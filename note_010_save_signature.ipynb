{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cppxaxa/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/cppxaxa/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/cppxaxa/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/cppxaxa/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/cppxaxa/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/cppxaxa/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/33997823/tensorflow-mlp-not-training-xor\n",
    "\n",
    "# create a placeholder for the input\n",
    "# None indicates a variable batch size for the input\n",
    "n_input = tf.placeholder(tf.float32, shape=[None, 2], name=\"n_input\")\n",
    "n_output = tf.placeholder(tf.float32, shape=[None, 1], name=\"n_output\")\n",
    "\n",
    "################\n",
    "# hidden layer #\n",
    "################\n",
    "\n",
    "layer1_count = 5\n",
    "layer1_b_hidden = tf.Variable(tf.random_normal([layer1_count]), name=\"layer1_hidden_bias\")\n",
    "layer1_W_hidden = tf.Variable(tf.random_normal([2, layer1_count]), name=\"layer1_hidden_weights\")\n",
    "layer1 = tf.sigmoid(tf.matmul(n_input, layer1_W_hidden) + layer1_b_hidden)\n",
    "\n",
    "################\n",
    "# output layer #\n",
    "################\n",
    "\n",
    "W_output = tf.Variable(tf.random_normal([layer1_count, 1]), name=\"output_weights\")  # output layer's weight matrix\n",
    "output = tf.sigmoid(tf.matmul(layer1, W_output))  # calc output layer's activation\n",
    "result = tf.identity(output, name=\"result\")\n",
    "\n",
    "############\n",
    "# learning #\n",
    "############\n",
    "\n",
    "cross_entropy = -(n_output * tf.log(output) + (1 - n_output) * tf.log(1 - output))\n",
    "# error = cross_entropy\n",
    "error = tf.square(n_output - output)  # simpler, but also works\n",
    "\n",
    "loss = tf.reduce_mean(error)  # mean the cross_entropy\n",
    "optimizer = tf.train.AdamOptimizer(0.01)  # take a gradient descent for optimizing with a \"stepsize\" of 0.1\n",
    "train = optimizer.minimize(loss)  # let the optimizer train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step:   0\n",
      "loss: 0.4670673906803131\n",
      "loss: 0.0002204965567216277\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: xor_010_ck/saved_model.pb\n",
      "\n",
      "Elapsed time  8.042747\n",
      "Epoch 400\n",
      "input: [0.0, 0.0] | output: [[0.01750605]]\n",
      "input: [0.0, 1.0] | output: [[0.9891367]]\n",
      "input: [1.0, 0.0] | output: [[0.97706455]]\n",
      "input: [1.0, 1.0] | output: [[0.02910046]]\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/33997823/tensorflow-mlp-not-training-xor\n",
    "# https://stackoverflow.com/a/47235448/6484802 - Signatures\n",
    "\n",
    "##############\n",
    "# checkpoint #\n",
    "##############\n",
    "\n",
    "model_path = 'xor_010_ck'\n",
    "if os.path.exists(model_path):\n",
    "    shutil.rmtree(model_path)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "saver = tf.saved_model.builder.SavedModelBuilder(model_path)\n",
    "\n",
    "####################\n",
    "# initialize graph #\n",
    "####################\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()  # create the session and therefore the graph\n",
    "sess.run(init)  # initialize all variables  \n",
    "\n",
    "\n",
    "###########\n",
    "# summary #\n",
    "###########\n",
    "\n",
    "writer = tf.summary.FileWriter(\"xor_010\", sess.graph)\n",
    "cost_summary_entity = tf.summary.scalar('loss', loss)\n",
    "fetch_summary = tf.summary.merge_all()\n",
    "\n",
    "#####################\n",
    "# train the network #\n",
    "#####################\n",
    "\n",
    "# define input and output data\n",
    "input_data = [[0., 0.], [0., 1.], [1., 0.], [1., 1.]]  # XOR input\n",
    "output_data = [[0.], [1.], [1.], [0.]]  # XOR output\n",
    "\n",
    "batch_xdata = [\n",
    "    [[0.19, 0.05]], [[0.04, 0.964]], [[0.956, 0.03]], [[0.899, 0.987]],\n",
    "    [[0.2, 0.89]], [[0.98, 0.91]], [[0.23, 0.01]], [[0.28, 0.99]],\n",
    "    [[0.93, 0.92]], [[0.88, 0.025]], [[0.18, 0.97]], [[0.11, 0.08]],\n",
    "    [[0.95, 0.09]], [[0.977, 0.991]], [[0.061, 0.091]], [[0.12, 0.955]]\n",
    "]\n",
    "batch_ydata = [\n",
    "    [[0.]], [[1.]], [[1.]], [[0.]],\n",
    "    [[1.]], [[0.]], [[0.]], [[1.]],\n",
    "    [[0.]], [[1.]], [[1.]], [[0.]],\n",
    "    [[1.]], [[0.]], [[0.]], [[1.]]\n",
    "]\n",
    "\n",
    "bak_epoch = 0\n",
    "t_start = time.clock()\n",
    "for epoch in range(0, 5000):\n",
    "    bak_epoch = epoch\n",
    "    # run the training operation\n",
    "    for i in range(len(batch_xdata)):\n",
    "        xdata = batch_xdata[i]\n",
    "        ydata = batch_ydata[i]\n",
    "        cvalues = sess.run([train, loss],\n",
    "                           feed_dict={n_input: xdata, n_output: ydata})\n",
    "\n",
    "    # print some debug\n",
    "    if epoch % 50 == 0:\n",
    "        summary_data = sess.run(fetch_summary, feed_dict={n_input: input_data, n_output: output_data})\n",
    "        writer.add_summary(summary_data, epoch)\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        loss_value = cvalues[1]\n",
    "        if loss_value < 0.002:\n",
    "            print(\"loss: {}\".format(loss_value))\n",
    "            break\n",
    "            \n",
    "    if epoch % 1000 == 0:\n",
    "        print(\"\")\n",
    "        print(\"step: {:>3}\".format(epoch))\n",
    "        print(\"loss: {}\".format(loss_value))\n",
    "        \n",
    "t_end = time.clock()\n",
    "\n",
    "tensor_info_x = tf.saved_model.utils.build_tensor_info(n_input)\n",
    "tensor_info_y = tf.saved_model.utils.build_tensor_info(result)\n",
    "\n",
    "prediction_signature = (\n",
    "    tf.saved_model.signature_def_utils.build_signature_def(\n",
    "        inputs={'x_input': tensor_info_x},\n",
    "        outputs={'y_output': tensor_info_y},\n",
    "        method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\n",
    "\n",
    "saver.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "    signature_def_map={\n",
    "        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n",
    "        prediction_signature \n",
    "    })\n",
    "saver.save()\n",
    "\n",
    "print(\"\")\n",
    "print('Elapsed time ', t_end - t_start)\n",
    "print(\"Epoch\", bak_epoch)\n",
    "for el in input_data:\n",
    "    print(\"input: {} | output: {}\".format(el, sess.run(output, feed_dict={n_input: [el]})))\n",
    "    \n",
    "sess.close()\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
